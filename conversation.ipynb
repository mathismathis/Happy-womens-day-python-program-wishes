{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathismathis/Happy-womens-day-python-program-wishes/blob/master/conversation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdFngJNhq50N"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Un4IrtJJruZz"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Conversation.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "RefKgh7Ir2SQ",
        "outputId": "69534a4f-91d4-493d-b953-be103e104973"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4503b8c6-59ed-44be-8350-60408068b905\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hi, how are you doing?</td>\n",
              "      <td>i'm fine. how about yourself?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i'm fine. how about yourself?</td>\n",
              "      <td>i'm pretty good. thanks for asking.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i'm pretty good. thanks for asking.</td>\n",
              "      <td>no problem. so how have you been?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>no problem. so how have you been?</td>\n",
              "      <td>i've been great. what about you?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>i've been great. what about you?</td>\n",
              "      <td>i've been good. i'm in school right now.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3720</th>\n",
              "      <td>that's a good question. maybe it's not old age.</td>\n",
              "      <td>are you right-handed?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3721</th>\n",
              "      <td>are you right-handed?</td>\n",
              "      <td>yes. all my life.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3722</th>\n",
              "      <td>yes. all my life.</td>\n",
              "      <td>you're wearing out your right hand. stop using...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3723</th>\n",
              "      <td>you're wearing out your right hand. stop using...</td>\n",
              "      <td>but i do all my writing with my right hand.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3724</th>\n",
              "      <td>but i do all my writing with my right hand.</td>\n",
              "      <td>start typing instead. that way your left hand ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3725 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4503b8c6-59ed-44be-8350-60408068b905')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4503b8c6-59ed-44be-8350-60408068b905 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4503b8c6-59ed-44be-8350-60408068b905');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               question  \\\n",
              "0                                hi, how are you doing?   \n",
              "1                         i'm fine. how about yourself?   \n",
              "2                   i'm pretty good. thanks for asking.   \n",
              "3                     no problem. so how have you been?   \n",
              "4                      i've been great. what about you?   \n",
              "...                                                 ...   \n",
              "3720    that's a good question. maybe it's not old age.   \n",
              "3721                              are you right-handed?   \n",
              "3722                                  yes. all my life.   \n",
              "3723  you're wearing out your right hand. stop using...   \n",
              "3724        but i do all my writing with my right hand.   \n",
              "\n",
              "                                                 answer  \n",
              "0                         i'm fine. how about yourself?  \n",
              "1                   i'm pretty good. thanks for asking.  \n",
              "2                     no problem. so how have you been?  \n",
              "3                      i've been great. what about you?  \n",
              "4              i've been good. i'm in school right now.  \n",
              "...                                                 ...  \n",
              "3720                              are you right-handed?  \n",
              "3721                                  yes. all my life.  \n",
              "3722  you're wearing out your right hand. stop using...  \n",
              "3723        but i do all my writing with my right hand.  \n",
              "3724  start typing instead. that way your left hand ...  \n",
              "\n",
              "[3725 rows x 2 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XNawPKq7UsL"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"hi, how are you doing?\",\n",
        "    \"i'm fine. how about yourself?\",\n",
        "    \"i'm pretty good. thanks for asking.\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    \"i'm fine. how about yourself?\",\n",
        "    \"i'm pretty good. thanks for asking.\",\n",
        "    \"no problem. so how have you been?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "0u1mrWHcZ-40",
        "outputId": "66d840b0-b7e4-4777-dda6-7465ea5ec4a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-0ef8c1883d63>\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_sequences_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_sequences_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Generate responses using the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 235, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'sequential_23' (type Sequential).\n    \n    Input 0 of layer \"lstm_13\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 5)\n    \n    Call arguments received by layer 'sequential_23' (type Sequential):\n      • inputs=tf.Tensor(shape=(None, 5), dtype=int32)\n      • training=True\n      • mask=None\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Define the questions and answers\n",
        "questions = [\"hi, how are you doing?\"]\n",
        "answers = [\"i'm fine. how about yourself?\"]\n",
        "\n",
        "# Tokenize the questions and answers\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert the questions and answers into numerical sequences\n",
        "question_sequences = tokenizer.texts_to_sequences(questions)\n",
        "answer_sequences = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "# Pad the sequences to have the same length\n",
        "max_sequence_length = max(len(seq) for seq in question_sequences + answer_sequences)\n",
        "question_sequences_padded = pad_sequences(question_sequences, maxlen=max_sequence_length, padding='post')\n",
        "answer_sequences_padded = pad_sequences(answer_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, input_shape=(max_sequence_length, vocab_size)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(question_sequences_padded, answer_sequences_padded, epochs=10, batch_size=32)\n",
        "\n",
        "# Generate responses using the trained model\n",
        "user_input = \"hi\"\n",
        "user_input_sequence = tokenizer.texts_to_sequences([user_input])\n",
        "user_input_padded = pad_sequences(user_input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "response_index = np.argmax(model.predict(user_input_padded), axis=-1)[0]\n",
        "response = tokenizer.index_word[response_index]\n",
        "\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "vuN8UaHutAb2",
        "outputId": "7c78d09f-52ba-40fb-ec89-4fd6ca8f5338"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-39-d7b3f44d767c>:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  model.fit(padded_input_sequences, np.array(response_sequences_idx), epochs=50)\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d7b3f44d767c>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_input_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_sequences_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Rest of the code remains the same...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Input sequences\n",
        "input_sequences = [\n",
        "    ['hi', 'how', 'are', 'you'],\n",
        "    ['hi', 'what', 'is', 'your', 'name'],\n",
        "    ['hi', 'how', 'is', 'the', 'weather', 'today'],\n",
        "    ['hi', 'can', 'you', 'help', 'me'],\n",
        "    ['hi', 'how', 'can', 'I', 'assist', 'you'],\n",
        "    ['hi', 'nice', 'to', 'meet', 'you']\n",
        "]\n",
        "\n",
        "# Target responses\n",
        "response_sequences = [\n",
        "    ['hello', 'I', 'am', 'fine'],\n",
        "    ['my', 'name', 'is', 'ChatGPT'],\n",
        "    ['the', 'weather', 'is', 'sunny', 'today'],\n",
        "    ['sure', 'what', 'do', 'you', 'need', 'help', 'with'],\n",
        "    ['I', 'can', 'help', 'you', 'with', 'various', 'topics'],\n",
        "    ['nice', 'to', 'meet', 'you', 'too']\n",
        "]\n",
        "\n",
        "# Create a vocabulary from the input and target sequences\n",
        "vocabulary = set()\n",
        "for sequence in input_sequences + response_sequences:\n",
        "    vocabulary.update(sequence)\n",
        "vocab_size = len(vocabulary) + 1\n",
        "word_to_index = {word: index + 1 for index, word in enumerate(vocabulary)}\n",
        "index_to_word = {index + 1: word for index, word in enumerate(vocabulary)}\n",
        "\n",
        "# Convert input and target sequences to index sequences\n",
        "input_sequences_idx = [[word_to_index[word] for word in sequence] for sequence in input_sequences]\n",
        "response_sequences_idx = [[word_to_index[word] for word in sequence] for sequence in response_sequences]\n",
        "\n",
        "# Pad sequences to a fixed length\n",
        "max_sequence_length = max(max(len(seq) for seq in input_sequences_idx), max(len(seq) for seq in response_sequences_idx))\n",
        "padded_input_sequences = pad_sequences(input_sequences_idx, maxlen=max_sequence_length)\n",
        "padded_response_sequences = pad_sequences(response_sequences_idx, maxlen=max_sequence_length)\n",
        "\n",
        "# Define the RNN model\n",
        "embedding_dim = 64\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(LSTM(units=128))\n",
        "model.add(Dense(units=vocab_size, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(padded_input_sequences, np.array(response_sequences_idx), epochs=50)\n",
        "\n",
        "# Rest of the code remains the same...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj76x4iIt4r_",
        "outputId": "c8ddff46-a2c5-4b15-d720-6abb955f4742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 24ms/step\n",
            "Bot: you\n"
          ]
        }
      ],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(input_texts + response_texts)\n",
        "max_sequence_length = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in response_sequences))\n",
        "\n",
        "# Define a function to generate responses\n",
        "def generate_response(input_text):\n",
        "    input_sequence = tokenizer.texts_to_sequences([input_text])\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "    response_sequence = model.predict(input_sequence)\n",
        "    response_sequence = np.argmax(response_sequence, axis=-1)[0]\n",
        "    response_text = tokenizer.sequences_to_texts([response_sequence])[0]\n",
        "    return response_text\n",
        "\n",
        "# Test the chatbot\n",
        "input_text = 'chat'\n",
        "response = generate_response(input_text)\n",
        "print('Bot:', response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "BQTXl0c7zw_7",
        "outputId": "6835834d-5419-4089-9230-71758558e74f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-94b858ecab46>\u001b[0m in \u001b[0;36m<cell line: 55>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m# Define the inference models for prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 1 of layer \"model\" is incompatible with the layer: expected shape=(None, 5), found shape=(1, 4)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the dataset\n",
        "input_texts = [\"Hi, how are you doing?\", \"What's your name?\", \"Goodbye!\"]\n",
        "target_texts = [\"I'm fine. How about yourself?\", \"My name is ChatBot.\", \"Goodbye! Have a nice day.\"]\n",
        "\n",
        "# Tokenize the input and target texts\n",
        "input_tokenizer = Tokenizer()\n",
        "target_tokenizer = Tokenizer()\n",
        "input_tokenizer.fit_on_texts(input_texts)\n",
        "target_tokenizer.fit_on_texts(target_texts)\n",
        "\n",
        "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
        "target_sequences = target_tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_length = max(max(len(seq) for seq in input_sequences), max(len(seq) for seq in target_sequences))\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_length, padding='post')\n",
        "target_sequences = pad_sequences(target_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Define vocabulary sizes\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
        "\n",
        "# Define the model architecture\n",
        "latent_dim = 64\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = Input(shape=(max_sequence_length,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(input_vocab_size, latent_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = Input(shape=(max_sequence_length,))\n",
        "decoder_embedding = tf.keras.layers.Embedding(target_vocab_size, latent_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "# Train the model\n",
        "model.fit([input_sequences, target_sequences[:, :-1]], target_sequences[:, 1:], batch_size=1, epochs=20)\n",
        "\n",
        "# Define the inference models for prediction\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n",
        "# Function to generate a response given an input sequence\n",
        "def generate_response(input_text):\n",
        "    input_sequence = input_tokenizer.texts_to_sequences([input_text])\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "    states_value = encoder_model.predict(input_sequence)\n",
        "\n",
        "    target_sequence = np.zeros((1, max_sequence_length))\n",
        "    target_sequence[0, 0] = target_tokenizer.word_index['<start>']\n",
        "\n",
        "    stop_condition = False\n",
        "    response = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_sequence] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = target_tokenizer.index_word[sampled_token_index]\n",
        "\n",
        "        if sampled_word != '<end>':\n",
        "            response += sampled_word + ' '\n",
        "\n",
        "        if sampled_word == '<end>' or len(response.split()) >= max_sequence_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        target_sequence[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "# Test the chatbot\n",
        "while True:\n",
        "    input_text = input(\"You: \")\n",
        "    response = generate_response(input_text)\n",
        "    print(\"Bot:\", response)\n",
        "    print()\n",
        "    if input_text.lower() == 'exit':\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJBF6VcpooK0",
        "outputId": "e70638f0-2bb7-42c7-99ad-0da90fab59de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19\n",
            "Epoch 1/700\n",
            "117/117 [==============================] - 35s 255ms/step - loss: 2.8824 - accuracy: 0.6566\n",
            "Epoch 2/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 2.3211 - accuracy: 0.6659\n",
            "Epoch 3/700\n",
            "117/117 [==============================] - 30s 251ms/step - loss: 2.2858 - accuracy: 0.6665\n",
            "Epoch 4/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 2.2570 - accuracy: 0.6666\n",
            "Epoch 5/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 2.2293 - accuracy: 0.6669\n",
            "Epoch 6/700\n",
            "117/117 [==============================] - 31s 269ms/step - loss: 2.2053 - accuracy: 0.6673\n",
            "Epoch 7/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 2.1847 - accuracy: 0.6677\n",
            "Epoch 8/700\n",
            "117/117 [==============================] - 29s 249ms/step - loss: 2.1659 - accuracy: 0.6682\n",
            "Epoch 9/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 2.1455 - accuracy: 0.6688\n",
            "Epoch 10/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 2.1258 - accuracy: 0.6690\n",
            "Epoch 11/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 2.1041 - accuracy: 0.6699\n",
            "Epoch 12/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 2.0829 - accuracy: 0.6708\n",
            "Epoch 13/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 2.0621 - accuracy: 0.6714\n",
            "Epoch 14/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 2.0447 - accuracy: 0.6718\n",
            "Epoch 15/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 2.0206 - accuracy: 0.6729\n",
            "Epoch 16/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 1.9960 - accuracy: 0.6739\n",
            "Epoch 17/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 1.9768 - accuracy: 0.6744\n",
            "Epoch 18/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 1.9524 - accuracy: 0.6754\n",
            "Epoch 19/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 1.9377 - accuracy: 0.6758\n",
            "Epoch 20/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 1.9086 - accuracy: 0.6761\n",
            "Epoch 21/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 1.8818 - accuracy: 0.6773\n",
            "Epoch 22/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 1.8582 - accuracy: 0.6777\n",
            "Epoch 23/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 1.8358 - accuracy: 0.6785\n",
            "Epoch 24/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 1.8091 - accuracy: 0.6801\n",
            "Epoch 25/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 1.7794 - accuracy: 0.6815\n",
            "Epoch 26/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 1.7550 - accuracy: 0.6825\n",
            "Epoch 27/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 1.7259 - accuracy: 0.6844\n",
            "Epoch 28/700\n",
            "117/117 [==============================] - 30s 251ms/step - loss: 1.6989 - accuracy: 0.6864\n",
            "Epoch 29/700\n",
            "117/117 [==============================] - 29s 249ms/step - loss: 1.6733 - accuracy: 0.6883\n",
            "Epoch 30/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 1.6452 - accuracy: 0.6908\n",
            "Epoch 31/700\n",
            "117/117 [==============================] - 31s 268ms/step - loss: 1.6143 - accuracy: 0.6941\n",
            "Epoch 32/700\n",
            "117/117 [==============================] - 30s 252ms/step - loss: 1.5852 - accuracy: 0.6968\n",
            "Epoch 33/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 1.5506 - accuracy: 0.6999\n",
            "Epoch 34/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 1.5190 - accuracy: 0.7040\n",
            "Epoch 35/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 1.4920 - accuracy: 0.7068\n",
            "Epoch 36/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 1.4622 - accuracy: 0.7114\n",
            "Epoch 37/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 1.4324 - accuracy: 0.7152\n",
            "Epoch 38/700\n",
            "117/117 [==============================] - 32s 275ms/step - loss: 1.4023 - accuracy: 0.7195\n",
            "Epoch 39/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 1.3708 - accuracy: 0.7231\n",
            "Epoch 40/700\n",
            "117/117 [==============================] - 29s 249ms/step - loss: 1.3445 - accuracy: 0.7275\n",
            "Epoch 41/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 1.3190 - accuracy: 0.7308\n",
            "Epoch 42/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 1.2890 - accuracy: 0.7355\n",
            "Epoch 43/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 1.2774 - accuracy: 0.7380\n",
            "Epoch 44/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 1.2417 - accuracy: 0.7433\n",
            "Epoch 45/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 1.2058 - accuracy: 0.7498\n",
            "Epoch 46/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 1.1804 - accuracy: 0.7545\n",
            "Epoch 47/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 1.1561 - accuracy: 0.7581\n",
            "Epoch 48/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 1.1309 - accuracy: 0.7621\n",
            "Epoch 49/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 1.1058 - accuracy: 0.7668\n",
            "Epoch 50/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 1.0779 - accuracy: 0.7715\n",
            "Epoch 51/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 1.0553 - accuracy: 0.7756\n",
            "Epoch 52/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 1.0384 - accuracy: 0.7786\n",
            "Epoch 53/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 1.0553 - accuracy: 0.7746\n",
            "Epoch 54/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 1.0090 - accuracy: 0.7841\n",
            "Epoch 55/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.9776 - accuracy: 0.7914\n",
            "Epoch 56/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.9535 - accuracy: 0.7960\n",
            "Epoch 57/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.9279 - accuracy: 0.8009\n",
            "Epoch 58/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.9079 - accuracy: 0.8048\n",
            "Epoch 59/700\n",
            "117/117 [==============================] - 30s 252ms/step - loss: 0.8881 - accuracy: 0.8088\n",
            "Epoch 60/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 0.8727 - accuracy: 0.8112\n",
            "Epoch 61/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 0.8559 - accuracy: 0.8152\n",
            "Epoch 62/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.8358 - accuracy: 0.8189\n",
            "Epoch 63/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.8186 - accuracy: 0.8222\n",
            "Epoch 64/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.8019 - accuracy: 0.8251\n",
            "Epoch 65/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.7918 - accuracy: 0.8278\n",
            "Epoch 66/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 0.7725 - accuracy: 0.8315\n",
            "Epoch 67/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.7578 - accuracy: 0.8348\n",
            "Epoch 68/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.7405 - accuracy: 0.8374\n",
            "Epoch 69/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.7229 - accuracy: 0.8414\n",
            "Epoch 70/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.7075 - accuracy: 0.8448\n",
            "Epoch 71/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.6932 - accuracy: 0.8483\n",
            "Epoch 72/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.6761 - accuracy: 0.8522\n",
            "Epoch 73/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.6633 - accuracy: 0.8548\n",
            "Epoch 74/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.6593 - accuracy: 0.8548\n",
            "Epoch 75/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.6697 - accuracy: 0.8513\n",
            "Epoch 76/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.6392 - accuracy: 0.8580\n",
            "Epoch 77/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.6141 - accuracy: 0.8642\n",
            "Epoch 78/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.5976 - accuracy: 0.8686\n",
            "Epoch 79/700\n",
            "117/117 [==============================] - 31s 260ms/step - loss: 0.5853 - accuracy: 0.8704\n",
            "Epoch 80/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.5723 - accuracy: 0.8731\n",
            "Epoch 81/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.5617 - accuracy: 0.8753\n",
            "Epoch 82/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.5512 - accuracy: 0.8783\n",
            "Epoch 83/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.5446 - accuracy: 0.8781\n",
            "Epoch 84/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.5363 - accuracy: 0.8798\n",
            "Epoch 85/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.5286 - accuracy: 0.8816\n",
            "Epoch 86/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.5159 - accuracy: 0.8846\n",
            "Epoch 87/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.5068 - accuracy: 0.8859\n",
            "Epoch 88/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.4929 - accuracy: 0.8885\n",
            "Epoch 89/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.4839 - accuracy: 0.8907\n",
            "Epoch 90/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.4755 - accuracy: 0.8926\n",
            "Epoch 91/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.4665 - accuracy: 0.8938\n",
            "Epoch 92/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.5039 - accuracy: 0.8862\n",
            "Epoch 93/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.4733 - accuracy: 0.8921\n",
            "Epoch 94/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.4581 - accuracy: 0.8943\n",
            "Epoch 95/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.4462 - accuracy: 0.8971\n",
            "Epoch 96/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.4300 - accuracy: 0.9007\n",
            "Epoch 97/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.4187 - accuracy: 0.9037\n",
            "Epoch 98/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.4374 - accuracy: 0.8988\n",
            "Epoch 99/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.4347 - accuracy: 0.8987\n",
            "Epoch 100/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.4112 - accuracy: 0.9048\n",
            "Epoch 101/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 0.3980 - accuracy: 0.9068\n",
            "Epoch 102/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.3880 - accuracy: 0.9083\n",
            "Epoch 103/700\n",
            "117/117 [==============================] - 32s 268ms/step - loss: 0.3794 - accuracy: 0.9114\n",
            "Epoch 104/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.3718 - accuracy: 0.9129\n",
            "Epoch 105/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.3662 - accuracy: 0.9130\n",
            "Epoch 106/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.3603 - accuracy: 0.9145\n",
            "Epoch 107/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 0.3539 - accuracy: 0.9153\n",
            "Epoch 108/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.3508 - accuracy: 0.9154\n",
            "Epoch 109/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.3456 - accuracy: 0.9173\n",
            "Epoch 110/700\n",
            "117/117 [==============================] - 31s 268ms/step - loss: 0.3425 - accuracy: 0.9166\n",
            "Epoch 111/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.3372 - accuracy: 0.9184\n",
            "Epoch 112/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.3318 - accuracy: 0.9192\n",
            "Epoch 113/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.3264 - accuracy: 0.9202\n",
            "Epoch 114/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.3221 - accuracy: 0.9214\n",
            "Epoch 115/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.3219 - accuracy: 0.9204\n",
            "Epoch 116/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.3225 - accuracy: 0.9202\n",
            "Epoch 117/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.3147 - accuracy: 0.9217\n",
            "Epoch 118/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 0.3099 - accuracy: 0.9224\n",
            "Epoch 119/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.3028 - accuracy: 0.9247\n",
            "Epoch 120/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.2946 - accuracy: 0.9260\n",
            "Epoch 121/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.2899 - accuracy: 0.9263\n",
            "Epoch 122/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.2855 - accuracy: 0.9269\n",
            "Epoch 123/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.2822 - accuracy: 0.9282\n",
            "Epoch 124/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.2802 - accuracy: 0.9282\n",
            "Epoch 125/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.2785 - accuracy: 0.9286\n",
            "Epoch 126/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.2863 - accuracy: 0.9269\n",
            "Epoch 127/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.2806 - accuracy: 0.9276\n",
            "Epoch 128/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.2712 - accuracy: 0.9298\n",
            "Epoch 129/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.2631 - accuracy: 0.9309\n",
            "Epoch 130/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.2583 - accuracy: 0.9323\n",
            "Epoch 131/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.2564 - accuracy: 0.9332\n",
            "Epoch 132/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.2589 - accuracy: 0.9323\n",
            "Epoch 133/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.2531 - accuracy: 0.9329\n",
            "Epoch 134/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.2548 - accuracy: 0.9330\n",
            "Epoch 135/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.2461 - accuracy: 0.9339\n",
            "Epoch 136/700\n",
            "117/117 [==============================] - 32s 271ms/step - loss: 0.2438 - accuracy: 0.9357\n",
            "Epoch 137/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.2362 - accuracy: 0.9369\n",
            "Epoch 138/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.2318 - accuracy: 0.9364\n",
            "Epoch 139/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.2283 - accuracy: 0.9379\n",
            "Epoch 140/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.2246 - accuracy: 0.9387\n",
            "Epoch 141/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.2202 - accuracy: 0.9397\n",
            "Epoch 142/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.2176 - accuracy: 0.9409\n",
            "Epoch 143/700\n",
            "117/117 [==============================] - 32s 271ms/step - loss: 0.2168 - accuracy: 0.9409\n",
            "Epoch 144/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.2127 - accuracy: 0.9411\n",
            "Epoch 145/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.2099 - accuracy: 0.9415\n",
            "Epoch 146/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.2069 - accuracy: 0.9425\n",
            "Epoch 147/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.2051 - accuracy: 0.9433\n",
            "Epoch 148/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.2026 - accuracy: 0.9442\n",
            "Epoch 149/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.2120 - accuracy: 0.9418\n",
            "Epoch 150/700\n",
            "117/117 [==============================] - 32s 272ms/step - loss: 0.2368 - accuracy: 0.9356\n",
            "Epoch 151/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 0.2206 - accuracy: 0.9394\n",
            "Epoch 152/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.2049 - accuracy: 0.9428\n",
            "Epoch 153/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.1988 - accuracy: 0.9454\n",
            "Epoch 154/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.2022 - accuracy: 0.9444\n",
            "Epoch 155/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.1948 - accuracy: 0.9458\n",
            "Epoch 156/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.1910 - accuracy: 0.9463\n",
            "Epoch 157/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.1799 - accuracy: 0.9491\n",
            "Epoch 158/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.1743 - accuracy: 0.9507\n",
            "Epoch 159/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.1730 - accuracy: 0.9510\n",
            "Epoch 160/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.1687 - accuracy: 0.9513\n",
            "Epoch 161/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.1653 - accuracy: 0.9527\n",
            "Epoch 162/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.1633 - accuracy: 0.9528\n",
            "Epoch 163/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.1608 - accuracy: 0.9532\n",
            "Epoch 164/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.1600 - accuracy: 0.9541\n",
            "Epoch 165/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.1570 - accuracy: 0.9549\n",
            "Epoch 166/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.1546 - accuracy: 0.9550\n",
            "Epoch 167/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.1537 - accuracy: 0.9552\n",
            "Epoch 168/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.1518 - accuracy: 0.9562\n",
            "Epoch 169/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.1490 - accuracy: 0.9568\n",
            "Epoch 170/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.1489 - accuracy: 0.9572\n",
            "Epoch 171/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.1503 - accuracy: 0.9566\n",
            "Epoch 172/700\n",
            "117/117 [==============================] - 30s 252ms/step - loss: 0.1480 - accuracy: 0.9571\n",
            "Epoch 173/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.1535 - accuracy: 0.9559\n",
            "Epoch 174/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.1614 - accuracy: 0.9553\n",
            "Epoch 175/700\n",
            "117/117 [==============================] - 31s 260ms/step - loss: 0.1547 - accuracy: 0.9562\n",
            "Epoch 176/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.1441 - accuracy: 0.9589\n",
            "Epoch 177/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.1371 - accuracy: 0.9600\n",
            "Epoch 178/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.1326 - accuracy: 0.9611\n",
            "Epoch 179/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.1293 - accuracy: 0.9622\n",
            "Epoch 180/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.1286 - accuracy: 0.9623\n",
            "Epoch 181/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.1254 - accuracy: 0.9621\n",
            "Epoch 182/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.1225 - accuracy: 0.9631\n",
            "Epoch 183/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.1207 - accuracy: 0.9639\n",
            "Epoch 184/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.1195 - accuracy: 0.9634\n",
            "Epoch 185/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.1174 - accuracy: 0.9646\n",
            "Epoch 186/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.1172 - accuracy: 0.9643\n",
            "Epoch 187/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.1156 - accuracy: 0.9644\n",
            "Epoch 188/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.1150 - accuracy: 0.9648\n",
            "Epoch 189/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.1190 - accuracy: 0.9645\n",
            "Epoch 190/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.1176 - accuracy: 0.9647\n",
            "Epoch 191/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.1267 - accuracy: 0.9625\n",
            "Epoch 192/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.2099 - accuracy: 0.9419\n",
            "Epoch 193/700\n",
            "117/117 [==============================] - 30s 252ms/step - loss: 0.1843 - accuracy: 0.9484\n",
            "Epoch 194/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.1406 - accuracy: 0.9589\n",
            "Epoch 195/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.1188 - accuracy: 0.9647\n",
            "Epoch 196/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.1096 - accuracy: 0.9664\n",
            "Epoch 197/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.1053 - accuracy: 0.9672\n",
            "Epoch 198/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.1025 - accuracy: 0.9676\n",
            "Epoch 199/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.1000 - accuracy: 0.9685\n",
            "Epoch 200/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.0978 - accuracy: 0.9686\n",
            "Epoch 201/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.0971 - accuracy: 0.9688\n",
            "Epoch 202/700\n",
            "117/117 [==============================] - 30s 261ms/step - loss: 0.0964 - accuracy: 0.9692\n",
            "Epoch 203/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0960 - accuracy: 0.9691\n",
            "Epoch 204/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0949 - accuracy: 0.9689\n",
            "Epoch 205/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0943 - accuracy: 0.9692\n",
            "Epoch 206/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0931 - accuracy: 0.9703\n",
            "Epoch 207/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0926 - accuracy: 0.9697\n",
            "Epoch 208/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.0915 - accuracy: 0.9700\n",
            "Epoch 209/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0912 - accuracy: 0.9697\n",
            "Epoch 210/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0906 - accuracy: 0.9698\n",
            "Epoch 211/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0900 - accuracy: 0.9704\n",
            "Epoch 212/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.0902 - accuracy: 0.9703\n",
            "Epoch 213/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0884 - accuracy: 0.9708\n",
            "Epoch 214/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0883 - accuracy: 0.9704\n",
            "Epoch 215/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0905 - accuracy: 0.9695\n",
            "Epoch 216/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.0907 - accuracy: 0.9698\n",
            "Epoch 217/700\n",
            "117/117 [==============================] - 30s 252ms/step - loss: 0.0928 - accuracy: 0.9701\n",
            "Epoch 218/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0965 - accuracy: 0.9689\n",
            "Epoch 219/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0983 - accuracy: 0.9686\n",
            "Epoch 220/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.1007 - accuracy: 0.9684\n",
            "Epoch 221/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.1001 - accuracy: 0.9683\n",
            "Epoch 222/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.0992 - accuracy: 0.9686\n",
            "Epoch 223/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0924 - accuracy: 0.9698\n",
            "Epoch 224/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.0879 - accuracy: 0.9697\n",
            "Epoch 225/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0871 - accuracy: 0.9705\n",
            "Epoch 226/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0843 - accuracy: 0.9709\n",
            "Epoch 227/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0798 - accuracy: 0.9714\n",
            "Epoch 228/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.0780 - accuracy: 0.9722\n",
            "Epoch 229/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.0780 - accuracy: 0.9715\n",
            "Epoch 230/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0778 - accuracy: 0.9720\n",
            "Epoch 231/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0774 - accuracy: 0.9721\n",
            "Epoch 232/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0776 - accuracy: 0.9718\n",
            "Epoch 233/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0768 - accuracy: 0.9719\n",
            "Epoch 234/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0773 - accuracy: 0.9712\n",
            "Epoch 235/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0772 - accuracy: 0.9717\n",
            "Epoch 236/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0765 - accuracy: 0.9719\n",
            "Epoch 237/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0775 - accuracy: 0.9713\n",
            "Epoch 238/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.1043 - accuracy: 0.9669\n",
            "Epoch 239/700\n",
            "117/117 [==============================] - 29s 250ms/step - loss: 0.1841 - accuracy: 0.9470\n",
            "Epoch 240/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.1181 - accuracy: 0.9627\n",
            "Epoch 241/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0928 - accuracy: 0.9687\n",
            "Epoch 242/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.0824 - accuracy: 0.9711\n",
            "Epoch 243/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0778 - accuracy: 0.9712\n",
            "Epoch 244/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0765 - accuracy: 0.9713\n",
            "Epoch 245/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0739 - accuracy: 0.9717\n",
            "Epoch 246/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0734 - accuracy: 0.9719\n",
            "Epoch 247/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0720 - accuracy: 0.9724\n",
            "Epoch 248/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0725 - accuracy: 0.9722\n",
            "Epoch 249/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.0722 - accuracy: 0.9716\n",
            "Epoch 250/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0723 - accuracy: 0.9723\n",
            "Epoch 251/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0726 - accuracy: 0.9723\n",
            "Epoch 252/700\n",
            "117/117 [==============================] - 32s 272ms/step - loss: 0.0715 - accuracy: 0.9723\n",
            "Epoch 253/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0716 - accuracy: 0.9728\n",
            "Epoch 254/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0719 - accuracy: 0.9725\n",
            "Epoch 255/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.0723 - accuracy: 0.9722\n",
            "Epoch 256/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.0735 - accuracy: 0.9722\n",
            "Epoch 257/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.0735 - accuracy: 0.9727\n",
            "Epoch 258/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0714 - accuracy: 0.9723\n",
            "Epoch 259/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0725 - accuracy: 0.9719\n",
            "Epoch 260/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0735 - accuracy: 0.9715\n",
            "Epoch 261/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0767 - accuracy: 0.9717\n",
            "Epoch 262/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0860 - accuracy: 0.9695\n",
            "Epoch 263/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.0963 - accuracy: 0.9673\n",
            "Epoch 264/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0906 - accuracy: 0.9691\n",
            "Epoch 265/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0789 - accuracy: 0.9711\n",
            "Epoch 266/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0729 - accuracy: 0.9720\n",
            "Epoch 267/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0723 - accuracy: 0.9717\n",
            "Epoch 268/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0709 - accuracy: 0.9727\n",
            "Epoch 269/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0699 - accuracy: 0.9721\n",
            "Epoch 270/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0695 - accuracy: 0.9722\n",
            "Epoch 271/700\n",
            "117/117 [==============================] - 30s 252ms/step - loss: 0.0689 - accuracy: 0.9720\n",
            "Epoch 272/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0683 - accuracy: 0.9722\n",
            "Epoch 273/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0687 - accuracy: 0.9722\n",
            "Epoch 274/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0682 - accuracy: 0.9726\n",
            "Epoch 275/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0679 - accuracy: 0.9728\n",
            "Epoch 276/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0701 - accuracy: 0.9723\n",
            "Epoch 277/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.0692 - accuracy: 0.9726\n",
            "Epoch 278/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0702 - accuracy: 0.9719\n",
            "Epoch 279/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0762 - accuracy: 0.9718\n",
            "Epoch 280/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0853 - accuracy: 0.9694\n",
            "Epoch 281/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0917 - accuracy: 0.9679\n",
            "Epoch 282/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0839 - accuracy: 0.9699\n",
            "Epoch 283/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.1060 - accuracy: 0.9678\n",
            "Epoch 284/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.1194 - accuracy: 0.9630\n",
            "Epoch 285/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0988 - accuracy: 0.9672\n",
            "Epoch 286/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0859 - accuracy: 0.9698\n",
            "Epoch 287/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0728 - accuracy: 0.9719\n",
            "Epoch 288/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0704 - accuracy: 0.9720\n",
            "Epoch 289/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0688 - accuracy: 0.9727\n",
            "Epoch 290/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0681 - accuracy: 0.9721\n",
            "Epoch 291/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.0678 - accuracy: 0.9723\n",
            "Epoch 292/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0670 - accuracy: 0.9722\n",
            "Epoch 293/700\n",
            "117/117 [==============================] - 31s 260ms/step - loss: 0.0670 - accuracy: 0.9724\n",
            "Epoch 294/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0677 - accuracy: 0.9718\n",
            "Epoch 295/700\n",
            "117/117 [==============================] - 29s 251ms/step - loss: 0.0670 - accuracy: 0.9732\n",
            "Epoch 296/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0668 - accuracy: 0.9723\n",
            "Epoch 297/700\n",
            "117/117 [==============================] - 29s 252ms/step - loss: 0.0668 - accuracy: 0.9729\n",
            "Epoch 298/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0666 - accuracy: 0.9720\n",
            "Epoch 299/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0663 - accuracy: 0.9725\n",
            "Epoch 300/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0670 - accuracy: 0.9720\n",
            "Epoch 301/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0666 - accuracy: 0.9728\n",
            "Epoch 302/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0670 - accuracy: 0.9723\n",
            "Epoch 303/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0663 - accuracy: 0.9723\n",
            "Epoch 304/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0672 - accuracy: 0.9722\n",
            "Epoch 305/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0668 - accuracy: 0.9721\n",
            "Epoch 306/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0670 - accuracy: 0.9724\n",
            "Epoch 307/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0668 - accuracy: 0.9726\n",
            "Epoch 308/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0671 - accuracy: 0.9722\n",
            "Epoch 309/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0683 - accuracy: 0.9720\n",
            "Epoch 310/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0683 - accuracy: 0.9720\n",
            "Epoch 311/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0706 - accuracy: 0.9721\n",
            "Epoch 312/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0729 - accuracy: 0.9713\n",
            "Epoch 313/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0781 - accuracy: 0.9706\n",
            "Epoch 314/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0862 - accuracy: 0.9687\n",
            "Epoch 315/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0852 - accuracy: 0.9689\n",
            "Epoch 316/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0768 - accuracy: 0.9708\n",
            "Epoch 317/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0744 - accuracy: 0.9714\n",
            "Epoch 318/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0690 - accuracy: 0.9718\n",
            "Epoch 319/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0672 - accuracy: 0.9719\n",
            "Epoch 320/700\n",
            "117/117 [==============================] - 31s 260ms/step - loss: 0.0654 - accuracy: 0.9723\n",
            "Epoch 321/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0652 - accuracy: 0.9722\n",
            "Epoch 322/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0645 - accuracy: 0.9721\n",
            "Epoch 323/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0645 - accuracy: 0.9726\n",
            "Epoch 324/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0640 - accuracy: 0.9728\n",
            "Epoch 325/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0644 - accuracy: 0.9725\n",
            "Epoch 326/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0648 - accuracy: 0.9715\n",
            "Epoch 327/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0645 - accuracy: 0.9724\n",
            "Epoch 328/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0646 - accuracy: 0.9723\n",
            "Epoch 329/700\n",
            "117/117 [==============================] - 32s 275ms/step - loss: 0.0644 - accuracy: 0.9727\n",
            "Epoch 330/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0645 - accuracy: 0.9728\n",
            "Epoch 331/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0654 - accuracy: 0.9724\n",
            "Epoch 332/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0667 - accuracy: 0.9725\n",
            "Epoch 333/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0665 - accuracy: 0.9722\n",
            "Epoch 334/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0663 - accuracy: 0.9725\n",
            "Epoch 335/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0668 - accuracy: 0.9719\n",
            "Epoch 336/700\n",
            "117/117 [==============================] - 32s 270ms/step - loss: 0.0669 - accuracy: 0.9722\n",
            "Epoch 337/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0673 - accuracy: 0.9718\n",
            "Epoch 338/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0722 - accuracy: 0.9714\n",
            "Epoch 339/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0908 - accuracy: 0.9674\n",
            "Epoch 340/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.1104 - accuracy: 0.9630\n",
            "Epoch 341/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.1049 - accuracy: 0.9646\n",
            "Epoch 342/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0809 - accuracy: 0.9693\n",
            "Epoch 343/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0704 - accuracy: 0.9726\n",
            "Epoch 344/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0657 - accuracy: 0.9721\n",
            "Epoch 345/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0641 - accuracy: 0.9722\n",
            "Epoch 346/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0635 - accuracy: 0.9727\n",
            "Epoch 347/700\n",
            "117/117 [==============================] - 30s 252ms/step - loss: 0.0633 - accuracy: 0.9725\n",
            "Epoch 348/700\n",
            "117/117 [==============================] - 30s 252ms/step - loss: 0.0638 - accuracy: 0.9720\n",
            "Epoch 349/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0633 - accuracy: 0.9726\n",
            "Epoch 350/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0632 - accuracy: 0.9727\n",
            "Epoch 351/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0633 - accuracy: 0.9722\n",
            "Epoch 352/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.0634 - accuracy: 0.9726\n",
            "Epoch 353/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0635 - accuracy: 0.9726\n",
            "Epoch 354/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0635 - accuracy: 0.9726\n",
            "Epoch 355/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0634 - accuracy: 0.9722\n",
            "Epoch 356/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0634 - accuracy: 0.9726\n",
            "Epoch 357/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0643 - accuracy: 0.9722\n",
            "Epoch 358/700\n",
            "117/117 [==============================] - 31s 269ms/step - loss: 0.0640 - accuracy: 0.9724\n",
            "Epoch 359/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0638 - accuracy: 0.9728\n",
            "Epoch 360/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0643 - accuracy: 0.9719\n",
            "Epoch 361/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0642 - accuracy: 0.9726\n",
            "Epoch 362/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0638 - accuracy: 0.9723\n",
            "Epoch 363/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0643 - accuracy: 0.9725\n",
            "Epoch 364/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0647 - accuracy: 0.9719\n",
            "Epoch 365/700\n",
            "117/117 [==============================] - 32s 275ms/step - loss: 0.0646 - accuracy: 0.9720\n",
            "Epoch 366/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0645 - accuracy: 0.9722\n",
            "Epoch 367/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0638 - accuracy: 0.9730\n",
            "Epoch 368/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0644 - accuracy: 0.9724\n",
            "Epoch 369/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0684 - accuracy: 0.9717\n",
            "Epoch 370/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.1194 - accuracy: 0.9620\n",
            "Epoch 371/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.1529 - accuracy: 0.9535\n",
            "Epoch 372/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.1055 - accuracy: 0.9647\n",
            "Epoch 373/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0742 - accuracy: 0.9713\n",
            "Epoch 374/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0675 - accuracy: 0.9720\n",
            "Epoch 375/700\n",
            "117/117 [==============================] - 31s 260ms/step - loss: 0.0646 - accuracy: 0.9720\n",
            "Epoch 376/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0632 - accuracy: 0.9722\n",
            "Epoch 377/700\n",
            "117/117 [==============================] - 30s 252ms/step - loss: 0.0625 - accuracy: 0.9728\n",
            "Epoch 378/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0625 - accuracy: 0.9721\n",
            "Epoch 379/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0621 - accuracy: 0.9729\n",
            "Epoch 380/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0622 - accuracy: 0.9723\n",
            "Epoch 381/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0625 - accuracy: 0.9723\n",
            "Epoch 382/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0617 - accuracy: 0.9728\n",
            "Epoch 383/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0625 - accuracy: 0.9723\n",
            "Epoch 384/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0616 - accuracy: 0.9726\n",
            "Epoch 385/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0621 - accuracy: 0.9723\n",
            "Epoch 386/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0621 - accuracy: 0.9730\n",
            "Epoch 387/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0623 - accuracy: 0.9724\n",
            "Epoch 388/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0632 - accuracy: 0.9725\n",
            "Epoch 389/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0627 - accuracy: 0.9727\n",
            "Epoch 390/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0627 - accuracy: 0.9719\n",
            "Epoch 391/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0629 - accuracy: 0.9719\n",
            "Epoch 392/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0625 - accuracy: 0.9723\n",
            "Epoch 393/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0633 - accuracy: 0.9729\n",
            "Epoch 394/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0630 - accuracy: 0.9725\n",
            "Epoch 395/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0633 - accuracy: 0.9723\n",
            "Epoch 396/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0636 - accuracy: 0.9724\n",
            "Epoch 397/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0637 - accuracy: 0.9721\n",
            "Epoch 398/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0628 - accuracy: 0.9728\n",
            "Epoch 399/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0634 - accuracy: 0.9725\n",
            "Epoch 400/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0642 - accuracy: 0.9727\n",
            "Epoch 401/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0652 - accuracy: 0.9716\n",
            "Epoch 402/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0674 - accuracy: 0.9719\n",
            "Epoch 403/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0711 - accuracy: 0.9712\n",
            "Epoch 404/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0728 - accuracy: 0.9710\n",
            "Epoch 405/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.1084 - accuracy: 0.9640\n",
            "Epoch 406/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0980 - accuracy: 0.9659\n",
            "Epoch 407/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0768 - accuracy: 0.9705\n",
            "Epoch 408/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0676 - accuracy: 0.9713\n",
            "Epoch 409/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0637 - accuracy: 0.9728\n",
            "Epoch 410/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0624 - accuracy: 0.9726\n",
            "Epoch 411/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0619 - accuracy: 0.9724\n",
            "Epoch 412/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0614 - accuracy: 0.9729\n",
            "Epoch 413/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0613 - accuracy: 0.9723\n",
            "Epoch 414/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0610 - accuracy: 0.9722\n",
            "Epoch 415/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0613 - accuracy: 0.9719\n",
            "Epoch 416/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0618 - accuracy: 0.9728\n",
            "Epoch 417/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0615 - accuracy: 0.9722\n",
            "Epoch 418/700\n",
            "117/117 [==============================] - 32s 270ms/step - loss: 0.0621 - accuracy: 0.9726\n",
            "Epoch 419/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0618 - accuracy: 0.9725\n",
            "Epoch 420/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0613 - accuracy: 0.9727\n",
            "Epoch 421/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0613 - accuracy: 0.9723\n",
            "Epoch 422/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0617 - accuracy: 0.9722\n",
            "Epoch 423/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0619 - accuracy: 0.9721\n",
            "Epoch 424/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0615 - accuracy: 0.9729\n",
            "Epoch 425/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0621 - accuracy: 0.9725\n",
            "Epoch 426/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0630 - accuracy: 0.9721\n",
            "Epoch 427/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0624 - accuracy: 0.9722\n",
            "Epoch 428/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0630 - accuracy: 0.9723\n",
            "Epoch 429/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0640 - accuracy: 0.9722\n",
            "Epoch 430/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0643 - accuracy: 0.9722\n",
            "Epoch 431/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0654 - accuracy: 0.9721\n",
            "Epoch 432/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0720 - accuracy: 0.9709\n",
            "Epoch 433/700\n",
            "117/117 [==============================] - 31s 268ms/step - loss: 0.0760 - accuracy: 0.9704\n",
            "Epoch 434/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0798 - accuracy: 0.9697\n",
            "Epoch 435/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0832 - accuracy: 0.9687\n",
            "Epoch 436/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0986 - accuracy: 0.9659\n",
            "Epoch 437/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0758 - accuracy: 0.9705\n",
            "Epoch 438/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0652 - accuracy: 0.9723\n",
            "Epoch 439/700\n",
            "117/117 [==============================] - 30s 252ms/step - loss: 0.0628 - accuracy: 0.9725\n",
            "Epoch 440/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0614 - accuracy: 0.9718\n",
            "Epoch 441/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0606 - accuracy: 0.9726\n",
            "Epoch 442/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0609 - accuracy: 0.9726\n",
            "Epoch 443/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0608 - accuracy: 0.9723\n",
            "Epoch 444/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0608 - accuracy: 0.9722\n",
            "Epoch 445/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0607 - accuracy: 0.9721\n",
            "Epoch 446/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0612 - accuracy: 0.9723\n",
            "Epoch 447/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0608 - accuracy: 0.9727\n",
            "Epoch 448/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0610 - accuracy: 0.9719\n",
            "Epoch 449/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0613 - accuracy: 0.9723\n",
            "Epoch 450/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0606 - accuracy: 0.9729\n",
            "Epoch 451/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0609 - accuracy: 0.9726\n",
            "Epoch 452/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0603 - accuracy: 0.9729\n",
            "Epoch 453/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0612 - accuracy: 0.9727\n",
            "Epoch 454/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0611 - accuracy: 0.9725\n",
            "Epoch 455/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0618 - accuracy: 0.9725\n",
            "Epoch 456/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0614 - accuracy: 0.9719\n",
            "Epoch 457/700\n",
            "117/117 [==============================] - 32s 273ms/step - loss: 0.0615 - accuracy: 0.9727\n",
            "Epoch 458/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0611 - accuracy: 0.9724\n",
            "Epoch 459/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0621 - accuracy: 0.9724\n",
            "Epoch 460/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0627 - accuracy: 0.9716\n",
            "Epoch 461/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0639 - accuracy: 0.9717\n",
            "Epoch 462/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0653 - accuracy: 0.9719\n",
            "Epoch 463/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0749 - accuracy: 0.9702\n",
            "Epoch 464/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0785 - accuracy: 0.9700\n",
            "Epoch 465/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0737 - accuracy: 0.9703\n",
            "Epoch 466/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0676 - accuracy: 0.9719\n",
            "Epoch 467/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0669 - accuracy: 0.9720\n",
            "Epoch 468/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0645 - accuracy: 0.9718\n",
            "Epoch 469/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0632 - accuracy: 0.9722\n",
            "Epoch 470/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0623 - accuracy: 0.9722\n",
            "Epoch 471/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0611 - accuracy: 0.9724\n",
            "Epoch 472/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0604 - accuracy: 0.9724\n",
            "Epoch 473/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0606 - accuracy: 0.9722\n",
            "Epoch 474/700\n",
            "117/117 [==============================] - 31s 268ms/step - loss: 0.0607 - accuracy: 0.9723\n",
            "Epoch 475/700\n",
            "117/117 [==============================] - 30s 253ms/step - loss: 0.0609 - accuracy: 0.9723\n",
            "Epoch 476/700\n",
            "117/117 [==============================] - 30s 261ms/step - loss: 0.0608 - accuracy: 0.9725\n",
            "Epoch 477/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0608 - accuracy: 0.9725\n",
            "Epoch 478/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0606 - accuracy: 0.9722\n",
            "Epoch 479/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0606 - accuracy: 0.9722\n",
            "Epoch 480/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0607 - accuracy: 0.9730\n",
            "Epoch 481/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0608 - accuracy: 0.9722\n",
            "Epoch 482/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0610 - accuracy: 0.9722\n",
            "Epoch 483/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0614 - accuracy: 0.9721\n",
            "Epoch 484/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0629 - accuracy: 0.9726\n",
            "Epoch 485/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0643 - accuracy: 0.9718\n",
            "Epoch 486/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0632 - accuracy: 0.9718\n",
            "Epoch 487/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0625 - accuracy: 0.9724\n",
            "Epoch 488/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0626 - accuracy: 0.9720\n",
            "Epoch 489/700\n",
            "117/117 [==============================] - 31s 268ms/step - loss: 0.0632 - accuracy: 0.9719\n",
            "Epoch 490/700\n",
            "117/117 [==============================] - 31s 269ms/step - loss: 0.0640 - accuracy: 0.9722\n",
            "Epoch 491/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0634 - accuracy: 0.9722\n",
            "Epoch 492/700\n",
            "117/117 [==============================] - 31s 269ms/step - loss: 0.0643 - accuracy: 0.9721\n",
            "Epoch 493/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0640 - accuracy: 0.9721\n",
            "Epoch 494/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0658 - accuracy: 0.9713\n",
            "Epoch 495/700\n",
            "117/117 [==============================] - 32s 275ms/step - loss: 0.0694 - accuracy: 0.9710\n",
            "Epoch 496/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0775 - accuracy: 0.9697\n",
            "Epoch 497/700\n",
            "117/117 [==============================] - 31s 269ms/step - loss: 0.0705 - accuracy: 0.9712\n",
            "Epoch 498/700\n",
            "117/117 [==============================] - 31s 268ms/step - loss: 0.0656 - accuracy: 0.9724\n",
            "Epoch 499/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0639 - accuracy: 0.9715\n",
            "Epoch 500/700\n",
            "117/117 [==============================] - 31s 268ms/step - loss: 0.0624 - accuracy: 0.9723\n",
            "Epoch 501/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0707 - accuracy: 0.9709\n",
            "Epoch 502/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0739 - accuracy: 0.9709\n",
            "Epoch 503/700\n",
            "117/117 [==============================] - 32s 270ms/step - loss: 0.0647 - accuracy: 0.9721\n",
            "Epoch 504/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0627 - accuracy: 0.9728\n",
            "Epoch 505/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0610 - accuracy: 0.9723\n",
            "Epoch 506/700\n",
            "117/117 [==============================] - 32s 273ms/step - loss: 0.0607 - accuracy: 0.9719\n",
            "Epoch 507/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.0608 - accuracy: 0.9725\n",
            "Epoch 508/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0603 - accuracy: 0.9721\n",
            "Epoch 509/700\n",
            "117/117 [==============================] - 31s 269ms/step - loss: 0.0600 - accuracy: 0.9724\n",
            "Epoch 510/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0602 - accuracy: 0.9718\n",
            "Epoch 511/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0601 - accuracy: 0.9727\n",
            "Epoch 512/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0607 - accuracy: 0.9729\n",
            "Epoch 513/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0608 - accuracy: 0.9726\n",
            "Epoch 514/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0607 - accuracy: 0.9720\n",
            "Epoch 515/700\n",
            "117/117 [==============================] - 32s 275ms/step - loss: 0.0609 - accuracy: 0.9727\n",
            "Epoch 516/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0603 - accuracy: 0.9727\n",
            "Epoch 517/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0607 - accuracy: 0.9725\n",
            "Epoch 518/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0605 - accuracy: 0.9723\n",
            "Epoch 519/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0606 - accuracy: 0.9721\n",
            "Epoch 520/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0604 - accuracy: 0.9724\n",
            "Epoch 521/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0603 - accuracy: 0.9726\n",
            "Epoch 522/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0606 - accuracy: 0.9725\n",
            "Epoch 523/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0604 - accuracy: 0.9723\n",
            "Epoch 524/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0608 - accuracy: 0.9724\n",
            "Epoch 525/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0610 - accuracy: 0.9724\n",
            "Epoch 526/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0616 - accuracy: 0.9725\n",
            "Epoch 527/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0615 - accuracy: 0.9726\n",
            "Epoch 528/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0623 - accuracy: 0.9718\n",
            "Epoch 529/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0645 - accuracy: 0.9721\n",
            "Epoch 530/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0659 - accuracy: 0.9717\n",
            "Epoch 531/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0794 - accuracy: 0.9693\n",
            "Epoch 532/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0943 - accuracy: 0.9665\n",
            "Epoch 533/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0775 - accuracy: 0.9694\n",
            "Epoch 534/700\n",
            "117/117 [==============================] - 31s 269ms/step - loss: 0.0720 - accuracy: 0.9710\n",
            "Epoch 535/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0781 - accuracy: 0.9697\n",
            "Epoch 536/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0681 - accuracy: 0.9716\n",
            "Epoch 537/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0694 - accuracy: 0.9709\n",
            "Epoch 538/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0631 - accuracy: 0.9721\n",
            "Epoch 539/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0602 - accuracy: 0.9724\n",
            "Epoch 540/700\n",
            "117/117 [==============================] - 31s 269ms/step - loss: 0.0602 - accuracy: 0.9724\n",
            "Epoch 541/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0598 - accuracy: 0.9725\n",
            "Epoch 542/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0593 - accuracy: 0.9728\n",
            "Epoch 543/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0595 - accuracy: 0.9720\n",
            "Epoch 544/700\n",
            "117/117 [==============================] - 31s 268ms/step - loss: 0.0593 - accuracy: 0.9727\n",
            "Epoch 545/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0592 - accuracy: 0.9727\n",
            "Epoch 546/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0595 - accuracy: 0.9730\n",
            "Epoch 547/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0600 - accuracy: 0.9720\n",
            "Epoch 548/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0596 - accuracy: 0.9729\n",
            "Epoch 549/700\n",
            "117/117 [==============================] - 32s 271ms/step - loss: 0.0598 - accuracy: 0.9725\n",
            "Epoch 550/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0596 - accuracy: 0.9730\n",
            "Epoch 551/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0598 - accuracy: 0.9725\n",
            "Epoch 552/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0599 - accuracy: 0.9724\n",
            "Epoch 553/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0602 - accuracy: 0.9719\n",
            "Epoch 554/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0600 - accuracy: 0.9725\n",
            "Epoch 555/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0602 - accuracy: 0.9722\n",
            "Epoch 556/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0603 - accuracy: 0.9726\n",
            "Epoch 557/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0599 - accuracy: 0.9724\n",
            "Epoch 558/700\n",
            "117/117 [==============================] - 32s 269ms/step - loss: 0.0599 - accuracy: 0.9719\n",
            "Epoch 559/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0599 - accuracy: 0.9729\n",
            "Epoch 560/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0600 - accuracy: 0.9723\n",
            "Epoch 561/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0603 - accuracy: 0.9721\n",
            "Epoch 562/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0604 - accuracy: 0.9721\n",
            "Epoch 563/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0601 - accuracy: 0.9724\n",
            "Epoch 564/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0604 - accuracy: 0.9722\n",
            "Epoch 565/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0609 - accuracy: 0.9729\n",
            "Epoch 566/700\n",
            "117/117 [==============================] - 31s 268ms/step - loss: 0.0619 - accuracy: 0.9728\n",
            "Epoch 567/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0616 - accuracy: 0.9725\n",
            "Epoch 568/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0609 - accuracy: 0.9725\n",
            "Epoch 569/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0608 - accuracy: 0.9719\n",
            "Epoch 570/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.0626 - accuracy: 0.9723\n",
            "Epoch 571/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0654 - accuracy: 0.9719\n",
            "Epoch 572/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0738 - accuracy: 0.9702\n",
            "Epoch 573/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0891 - accuracy: 0.9667\n",
            "Epoch 574/700\n",
            "117/117 [==============================] - 31s 269ms/step - loss: 0.0859 - accuracy: 0.9678\n",
            "Epoch 575/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0711 - accuracy: 0.9710\n",
            "Epoch 576/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0639 - accuracy: 0.9721\n",
            "Epoch 577/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0606 - accuracy: 0.9723\n",
            "Epoch 578/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0594 - accuracy: 0.9728\n",
            "Epoch 579/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0594 - accuracy: 0.9731\n",
            "Epoch 580/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0588 - accuracy: 0.9724\n",
            "Epoch 581/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0590 - accuracy: 0.9722\n",
            "Epoch 582/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0589 - accuracy: 0.9725\n",
            "Epoch 583/700\n",
            "117/117 [==============================] - 31s 269ms/step - loss: 0.0590 - accuracy: 0.9728\n",
            "Epoch 584/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0591 - accuracy: 0.9723\n",
            "Epoch 585/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0590 - accuracy: 0.9725\n",
            "Epoch 586/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0591 - accuracy: 0.9725\n",
            "Epoch 587/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0591 - accuracy: 0.9727\n",
            "Epoch 588/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0591 - accuracy: 0.9723\n",
            "Epoch 589/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0597 - accuracy: 0.9723\n",
            "Epoch 590/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0598 - accuracy: 0.9720\n",
            "Epoch 591/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0595 - accuracy: 0.9724\n",
            "Epoch 592/700\n",
            "117/117 [==============================] - 32s 271ms/step - loss: 0.0594 - accuracy: 0.9730\n",
            "Epoch 593/700\n",
            "117/117 [==============================] - 32s 275ms/step - loss: 0.0592 - accuracy: 0.9722\n",
            "Epoch 594/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0596 - accuracy: 0.9724\n",
            "Epoch 595/700\n",
            "117/117 [==============================] - 32s 271ms/step - loss: 0.0594 - accuracy: 0.9724\n",
            "Epoch 596/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0591 - accuracy: 0.9728\n",
            "Epoch 597/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0591 - accuracy: 0.9729\n",
            "Epoch 598/700\n",
            "117/117 [==============================] - 32s 273ms/step - loss: 0.0595 - accuracy: 0.9733\n",
            "Epoch 599/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0598 - accuracy: 0.9730\n",
            "Epoch 600/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0599 - accuracy: 0.9731\n",
            "Epoch 601/700\n",
            "117/117 [==============================] - 33s 281ms/step - loss: 0.0605 - accuracy: 0.9719\n",
            "Epoch 602/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0605 - accuracy: 0.9729\n",
            "Epoch 603/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0601 - accuracy: 0.9734\n",
            "Epoch 604/700\n",
            "117/117 [==============================] - 31s 268ms/step - loss: 0.0602 - accuracy: 0.9728\n",
            "Epoch 605/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0600 - accuracy: 0.9723\n",
            "Epoch 606/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0613 - accuracy: 0.9724\n",
            "Epoch 607/700\n",
            "117/117 [==============================] - 31s 268ms/step - loss: 0.0638 - accuracy: 0.9722\n",
            "Epoch 608/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0748 - accuracy: 0.9695\n",
            "Epoch 609/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0812 - accuracy: 0.9690\n",
            "Epoch 610/700\n",
            "117/117 [==============================] - 33s 281ms/step - loss: 0.0777 - accuracy: 0.9695\n",
            "Epoch 611/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0665 - accuracy: 0.9719\n",
            "Epoch 612/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.0632 - accuracy: 0.9724\n",
            "Epoch 613/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0621 - accuracy: 0.9727\n",
            "Epoch 614/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0604 - accuracy: 0.9723\n",
            "Epoch 615/700\n",
            "117/117 [==============================] - 30s 260ms/step - loss: 0.0592 - accuracy: 0.9727\n",
            "Epoch 616/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0592 - accuracy: 0.9719\n",
            "Epoch 617/700\n",
            "117/117 [==============================] - 30s 261ms/step - loss: 0.0589 - accuracy: 0.9719\n",
            "Epoch 618/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0584 - accuracy: 0.9730\n",
            "Epoch 619/700\n",
            "117/117 [==============================] - 32s 274ms/step - loss: 0.0585 - accuracy: 0.9728\n",
            "Epoch 620/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0589 - accuracy: 0.9721\n",
            "Epoch 621/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0589 - accuracy: 0.9723\n",
            "Epoch 622/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0589 - accuracy: 0.9732\n",
            "Epoch 623/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0589 - accuracy: 0.9722\n",
            "Epoch 624/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0592 - accuracy: 0.9721\n",
            "Epoch 625/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0585 - accuracy: 0.9727\n",
            "Epoch 626/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0590 - accuracy: 0.9722\n",
            "Epoch 627/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0588 - accuracy: 0.9728\n",
            "Epoch 628/700\n",
            "117/117 [==============================] - 32s 275ms/step - loss: 0.0591 - accuracy: 0.9725\n",
            "Epoch 629/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0592 - accuracy: 0.9721\n",
            "Epoch 630/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0592 - accuracy: 0.9728\n",
            "Epoch 631/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0591 - accuracy: 0.9722\n",
            "Epoch 632/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0595 - accuracy: 0.9728\n",
            "Epoch 633/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0595 - accuracy: 0.9724\n",
            "Epoch 634/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0592 - accuracy: 0.9729\n",
            "Epoch 635/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.0597 - accuracy: 0.9723\n",
            "Epoch 636/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0590 - accuracy: 0.9727\n",
            "Epoch 637/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0595 - accuracy: 0.9723\n",
            "Epoch 638/700\n",
            "117/117 [==============================] - 31s 260ms/step - loss: 0.0598 - accuracy: 0.9730\n",
            "Epoch 639/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0598 - accuracy: 0.9722\n",
            "Epoch 640/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0601 - accuracy: 0.9731\n",
            "Epoch 641/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0626 - accuracy: 0.9722\n",
            "Epoch 642/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0909 - accuracy: 0.9667\n",
            "Epoch 643/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.1172 - accuracy: 0.9604\n",
            "Epoch 644/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0783 - accuracy: 0.9691\n",
            "Epoch 645/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0639 - accuracy: 0.9717\n",
            "Epoch 646/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0598 - accuracy: 0.9731\n",
            "Epoch 647/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0589 - accuracy: 0.9723\n",
            "Epoch 648/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0583 - accuracy: 0.9729\n",
            "Epoch 649/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0584 - accuracy: 0.9726\n",
            "Epoch 650/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0584 - accuracy: 0.9726\n",
            "Epoch 651/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0582 - accuracy: 0.9729\n",
            "Epoch 652/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0584 - accuracy: 0.9734\n",
            "Epoch 653/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0584 - accuracy: 0.9726\n",
            "Epoch 654/700\n",
            "117/117 [==============================] - 31s 263ms/step - loss: 0.0584 - accuracy: 0.9730\n",
            "Epoch 655/700\n",
            "117/117 [==============================] - 31s 260ms/step - loss: 0.0584 - accuracy: 0.9725\n",
            "Epoch 656/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0581 - accuracy: 0.9729\n",
            "Epoch 657/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0584 - accuracy: 0.9724\n",
            "Epoch 658/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0586 - accuracy: 0.9728\n",
            "Epoch 659/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0584 - accuracy: 0.9728\n",
            "Epoch 660/700\n",
            "117/117 [==============================] - 31s 261ms/step - loss: 0.0585 - accuracy: 0.9721\n",
            "Epoch 661/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0586 - accuracy: 0.9724\n",
            "Epoch 662/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0583 - accuracy: 0.9728\n",
            "Epoch 663/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0586 - accuracy: 0.9725\n",
            "Epoch 664/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0586 - accuracy: 0.9725\n",
            "Epoch 665/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0591 - accuracy: 0.9721\n",
            "Epoch 666/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0591 - accuracy: 0.9721\n",
            "Epoch 667/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0587 - accuracy: 0.9728\n",
            "Epoch 668/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0589 - accuracy: 0.9725\n",
            "Epoch 669/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0590 - accuracy: 0.9725\n",
            "Epoch 670/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0591 - accuracy: 0.9723\n",
            "Epoch 671/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0588 - accuracy: 0.9731\n",
            "Epoch 672/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0587 - accuracy: 0.9727\n",
            "Epoch 673/700\n",
            "117/117 [==============================] - 31s 268ms/step - loss: 0.0595 - accuracy: 0.9728\n",
            "Epoch 674/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0589 - accuracy: 0.9727\n",
            "Epoch 675/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0592 - accuracy: 0.9720\n",
            "Epoch 676/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0591 - accuracy: 0.9729\n",
            "Epoch 677/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0597 - accuracy: 0.9724\n",
            "Epoch 678/700\n",
            "117/117 [==============================] - 31s 265ms/step - loss: 0.0591 - accuracy: 0.9725\n",
            "Epoch 679/700\n",
            "117/117 [==============================] - 30s 254ms/step - loss: 0.0597 - accuracy: 0.9723\n",
            "Epoch 680/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0628 - accuracy: 0.9721\n",
            "Epoch 681/700\n",
            "117/117 [==============================] - 30s 258ms/step - loss: 0.0741 - accuracy: 0.9700\n",
            "Epoch 682/700\n",
            "117/117 [==============================] - 32s 273ms/step - loss: 0.0944 - accuracy: 0.9657\n",
            "Epoch 683/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0933 - accuracy: 0.9660\n",
            "Epoch 684/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0708 - accuracy: 0.9705\n",
            "Epoch 685/700\n",
            "117/117 [==============================] - 31s 266ms/step - loss: 0.0652 - accuracy: 0.9722\n",
            "Epoch 686/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0613 - accuracy: 0.9723\n",
            "Epoch 687/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0602 - accuracy: 0.9729\n",
            "Epoch 688/700\n",
            "117/117 [==============================] - 31s 267ms/step - loss: 0.0594 - accuracy: 0.9726\n",
            "Epoch 689/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0588 - accuracy: 0.9727\n",
            "Epoch 690/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0581 - accuracy: 0.9731\n",
            "Epoch 691/700\n",
            "117/117 [==============================] - 32s 276ms/step - loss: 0.0581 - accuracy: 0.9724\n",
            "Epoch 692/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0581 - accuracy: 0.9724\n",
            "Epoch 693/700\n",
            "117/117 [==============================] - 30s 255ms/step - loss: 0.0577 - accuracy: 0.9727\n",
            "Epoch 694/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0577 - accuracy: 0.9728\n",
            "Epoch 695/700\n",
            "117/117 [==============================] - 31s 262ms/step - loss: 0.0580 - accuracy: 0.9727\n",
            "Epoch 696/700\n",
            "117/117 [==============================] - 30s 256ms/step - loss: 0.0579 - accuracy: 0.9725\n",
            "Epoch 697/700\n",
            "117/117 [==============================] - 30s 259ms/step - loss: 0.0582 - accuracy: 0.9725\n",
            "Epoch 698/700\n",
            "117/117 [==============================] - 31s 264ms/step - loss: 0.0582 - accuracy: 0.9723\n",
            "Epoch 699/700\n",
            "117/117 [==============================] - 30s 257ms/step - loss: 0.0581 - accuracy: 0.9728\n",
            "Epoch 700/700\n",
            "117/117 [==============================] - 32s 271ms/step - loss: 0.0584 - accuracy: 0.9714\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense,Embedding\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pickle\n",
        "\n",
        "# Load and preprocess the data\n",
        "df = pd.read_csv('Conversation.csv')  # Replace 'Conversation.csv' with the path to your DataFrame file\n",
        "\n",
        "questions = df['question'].tolist()\n",
        "answers = df['answer'].tolist()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "\n",
        "question_sequences = tokenizer.texts_to_sequences(questions)\n",
        "answer_sequences = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "max_sequence_length = max(max(len(seq) for seq in question_sequences), max(len(seq) for seq in answer_sequences))\n",
        "print(max_sequence_length)\n",
        "question_sequences = pad_sequences(question_sequences, maxlen=max_sequence_length, padding='post')\n",
        "answer_sequences = pad_sequences(answer_sequences, maxlen=max_sequence_length, padding='post')\n",
        "\n",
        "# Define the encoder model\n",
        "encoder_inputs = Input(shape=(max_sequence_length,))\n",
        "encoder_embedding = Embedding(len(tokenizer.word_index) + 1, 100)(encoder_inputs)\n",
        "encoder_lstm = LSTM(256, return_state=True)\n",
        "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Define the decoder model\n",
        "decoder_inputs = Input(shape=(max_sequence_length,))\n",
        "decoder_embedding = Embedding(len(tokenizer.word_index) + 1, 100)(decoder_inputs)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(tokenizer.word_index) + 1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the Seq2Seq model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit([question_sequences, question_sequences], np.expand_dims(answer_sequences, -1), epochs=700, batch_size=32)\n",
        "\n",
        "\n",
        "# Save the trained model\n",
        "model.save('chatbot_model.h5')\n",
        "\n",
        "# Save the tokenizer\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "# # Generate responses from the trained model\n",
        "# def generate_response(input_text):\n",
        "#     input_sequence = tokenizer.texts_to_sequences([input_text])\n",
        "#     input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "#     predicted_sequence = model.predict([input_sequence, input_sequence]).argmax(axis=-1)\n",
        "#     return tokenizer.sequences_to_texts(predicted_sequence)[0]\n",
        "\n",
        "# # Test the chatbot\n",
        "# user_input = \"hi, how are you doing?\"\n",
        "# response = generate_response(user_input)\n",
        "# print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIQLbdmHxWpt",
        "outputId": "aea3f884-c20b-4afa-f8a0-77913c0d83b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fd4f6ccef80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 805ms/step\n",
            "i'm tried don't say some\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "\n",
        "loaded_model = load_model('chatbot_model.h5')\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    loaded_tokenizer = pickle.load(handle)\n",
        "\n",
        "# Generate responses from the loaded model\n",
        "def generate_response(input_text):\n",
        "    input_sequence = loaded_tokenizer.texts_to_sequences([input_text])\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "    predicted_sequence = loaded_model.predict([input_sequence, input_sequence]).argmax(axis=-1)\n",
        "    return loaded_tokenizer.sequences_to_texts(predicted_sequence)[0]\n",
        "\n",
        "# Test the chatbot\n",
        "user_input = \"HI hello this is mathddddddddddddddddddddddddddddddddddis\"\n",
        "response = generate_response(user_input)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aKuRr71w9nd"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "loaded_model = load_model('chatbot_model.h5')\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    loaded_tokenizer = pickle.load(handle)\n",
        "\n",
        "# Generate responses from the loaded model\n",
        "def generate_response(input_text):\n",
        "    input_sequence = loaded_tokenizer.texts_to_sequences([input_text])\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=max_sequence_length, padding='post')\n",
        "    predicted_sequence = loaded_model.predict([input_sequence, input_sequence]).argmax(axis=-1)\n",
        "    return loaded_tokenizer.sequences_to_texts(predicted_sequence)[0]\n",
        "\n",
        "# Test the chatbot\n",
        "user_input = \"hi, how are you doing?\"\n",
        "response = generate_response(user_input)\n",
        "print(response)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2dURaB1cd6Fg1ZWXoQVVi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}